{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node Classification with Graph Neural Networks\n",
    "\n",
    "<center><img src=https://imgs.search.brave.com/7L7HnDH6XNwidjkPapYJ4X8c2QvlwdThymMjA3D7PDY/rs:fit:1182:225:1/g:ce/aHR0cHM6Ly90c2U0/Lm1tLmJpbmcubmV0/L3RoP2lkPU9JUC53/bFowNFZBU2hSTkJ3/R09hWThWcXR3SGFD/LSZwaWQ9QXBp></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's install all the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "%pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "%pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "%pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TSNE** (t-distributed stochastic neighbor embedding) is a popular nonlinear dimensionality reduction technique often used for visualizing high-dimensional data in a low-dimensional space. It is particularly useful for visualizing embeddings learned from deep learning models. It works by constructing a probability distribution over pairs of high-dimensional objects in such a way that similar objects have a high probability of being picked, and then constructing a probability distribution over pairs of low-dimensional points such that the KL divergence between the two distributions is minimized. Finally, it optimizes the low-dimensional embeddings such that they reproduce the pairwise similarities in the high-dimensional space as closely as possible. The result is a set of low-dimensional points that can be plotted in two or three dimensions for easy visualization.\n",
    "\n",
    "[Previous: Introduction: Hands-on Graph Neural Networks](https://github.com/ramzan-basheer/Graph-Neural-Networks/blob/master/Introduction-Graph%20Neural%20Networks.ipynb)\n",
    "\n",
    "This tutorial will teach you how to apply **Graph Neural Networks (GNNs) to the task of node classification**.\n",
    "Here, we are given the ground-truth labels of only a small subset of nodes, and want to infer the labels for all the remaining nodes (*transductive learning*).\n",
    "\n",
    "To demonstrate, we make use of the `Cora` dataset, which is a **citation network** where nodes represent documents.\n",
    "Each node is described by a 1433-dimensional bag-of-words feature vector.\n",
    "Two documents are connected if there exists a citation link between them.\n",
    "The task is to infer the category of each document (7 in total).\n",
    "\n",
    "This dataset was first introduced by [Yang et al. (2016)](https://arxiv.org/abs/1603.08861) as one of the datasets of the `Planetoid` benchmark suite.\n",
    "We again can make use [PyTorch Geometric](https://github.com/rusty1s/pytorch_geometric) for an easy access to this dataset via [`torch_geometric.datasets.Planetoid`](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.Planetoid):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset provides the following information:\n",
    "\n",
    "* `x`: a feature matrix of size `(num_nodes, num_features)` containing bag-of-words representations of the content of each document.\n",
    "* `edge_index`: an edge index of size `(2, num_edges)` where `edge_index[0]` and `edge_index[1]` contain the indices of the start and end nodes of each edge, respectively.\n",
    "* `y`: a class label for each node, represented as an integer between **0** and `num_classes-1`.\n",
    "* `train_mask`: a binary mask of size `(num_nodes,)` that indicates which nodes should be used for training.\n",
    "* `val_mask`: a binary mask of size `(num_nodes,)` that indicates which nodes should be used for validation.\n",
    "* `test_mask`: a binary mask of size `(num_nodes,)` that indicates which nodes should be used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.transforms import NormalizeFeatures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `NormalizeFeatures` class in the `torch_geometric.transforms` module is used to normalize the node feature matrix of a graph dataset. Specifically, it normalizes the feature matrix by subtracting the mean and dividing by the standard deviation of each feature dimension across all nodes in the graph. Normalizing the node features can help improve the performance of machine learning models that operate on the graph, as it makes the features more comparable across different nodes and reduces the influence of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset = Planetoid(root='data/Planetoid', name='Cora', transform=NormalizeFeatures())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet loads the Cora dataset from the 'data/Planetoid' directory, normalizes its node features using the NormalizeFeatures transform, and returns a Data object containing the normalized graph and node labels.\n",
    "\n",
    "The Planetoid class is used to load the Cora dataset, which is a citation network dataset consisting of 2,708 scientific publications classified into one of seven classes. Each publication is represented by a feature vector of 1433 dimensions, which includes binary values indicating the presence or absence of certain words in the publication. The task is to classify each publication into one of the seven categories based on its feature vector and citation links to other publications.\n",
    "\n",
    "The resulting Data object contains the normalized feature matrix as `data.x`, the node labels as `data.y`, and the graph structure as `data.edge_index` (an edge index tensor in COO format). It can be used as input to various graph machine learning models in the PyTorch Geometric library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: Cora():\n",
      "======================\n",
      "Number of graphs: 1\n",
      "Number of features: 1433\n",
      "Number of classes: 7\n",
      "\n",
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n",
      "===========================================================================================================\n",
      "Number of nodes: 2708\n",
      "Number of edges: 10556\n",
      "Average node degree: 3.90\n",
      "Number of training nodes: 140\n",
      "Training node label rate: 0.05\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('===========================================================================================================')\n",
    "\n",
    "# Gather some statistics about the graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, this dataset is quite similar to the previously used [`KarateClub`](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.KarateClub) network.\n",
    "We can see that the `Cora` network holds 2,708 nodes and 10,556 edges, resulting in an average node degree of 3.9.\n",
    "For training this dataset, we are given the ground-truth categories of 140 nodes (20 for each class).\n",
    "This results in a training node label rate of only 5%.\n",
    "\n",
    "In contrast to `KarateClub`, this graph holds the additional attributes `val_mask` and `test_mask`, which denotes which nodes should be used for validation and testing.\n",
    "Furthermore, we make use of **[data transformations](https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html#data-transforms) via `transform=NormalizeFeatures()`**.\n",
    "Transforms can be used to modify your input data before inputting them into a neural network, *e.g.*, for normalization or data augmentation.\n",
    "Here, we [row-normalize](https://pytorch-geometric.readthedocs.io/en/latest/modules/transforms.html#torch_geometric.transforms.NormalizeFeatures) the bag-of-words input feature vectors.\n",
    "\n",
    "We can further see that this network is undirected, and that there exists no isolated nodes (each document has at least one citation)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Multi-layer Perception Network (MLP)\n",
    "\n",
    "In theory, we should be able to infer the category of a document solely based on its content, *i.e.* its bag-of-words feature representation, without taking any relational information into account.\n",
    "\n",
    "Let's verify that by constructing a simple MLP that solely operates on input node features (using shared weights across all nodes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.nn.functional` is a module in PyTorch that contains a large number of functions that are commonly used in neural network training and evaluation. These functions are designed to be used in conjunction with other PyTorch modules such as `torch.nn.Module`, which provides a way to define neural network layers and architectures.\n",
    "\n",
    "Here are some examples of the kinds of functions you can find in `torch.nn.functional`:\n",
    "\n",
    "* Activation functions such as `relu`, `sigmoid`, and `tanh`.\n",
    "* Pooling operations such as `max_pool2d` and `avg_pool2d`.\n",
    "* Loss functions such as `cross_entropy`, `mse_loss`, and `l1_loss`.\n",
    "* Convolutional operations such as `conv2d` and `conv_transpose2d`.\n",
    "* Normalization operations such as `batch_norm` and `instance_norm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.lin1 = Linear(dataset.num_features, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The `MLP` class extends `torch.nn.Module`, which is the base class for all PyTorch neural network modules.\n",
    "* The constructor for the MLP class takes a single argument `hidden_channels`, which specifies the number of hidden units in the MLP. This value is used to define the sizes of the linear layers in the network.\n",
    "* In the constructor, `super().__init__()` calls the constructor of the base class to initialize the module.\n",
    "* The `torch.manual_seed(12345)` line sets the random seed for reproducibility.\n",
    "* `self.lin1 = Linear(dataset.num_features, hidden_channels)` creates a Linear layer with `dataset.num_features` input channels and `hidden_channels` output channels. This layer performs a linear transformation of the input data, multiplying it by a weight matrix and adding a bias vector. `dataset.num_features` is the number of input features in the dataset.\n",
    "* `self.lin2 = Linear(hidden_channels, dataset.num_classes)` creates another Linear layer that takes `hidden_channels` input channels and produces `dataset.num_classes` output channels. This layer produces the final output of the network, which is a probability distribution over the possible classes in the dataset.\n",
    "* In the `forward` method, `x` is passed through the first linear layer with `x = self.lin1(x)`. The `relu` activation function is then applied to the output of this layer with `x = x.relu()`. This introduces nonlinearity into the model and allows it to learn more complex relationships between the input and output.\n",
    "* `F.dropout(x, p=0.5, training=self.training)` applies dropout regularization to the output of the first layer with a dropout probability of **0.5**. Dropout randomly \"drops out\" a fraction of the units in the layer during training, which can prevent overfitting and improve generalization performance.\n",
    "* The output of the first layer is then passed through the second linear layer with `x = self.lin2(x)`.\n",
    "* Finally, the output of the second layer is returned as the output of the `forward` method with return `x`.\n",
    "\n",
    "We define a simple two-layer MLP that takes an input vector, performs a linear transformation followed by a nonlinear activation function and dropout, and produces a probability distribution over the possible output classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (lin1): Linear(in_features=1433, out_features=16, bias=True)\n",
      "  (lin2): Linear(in_features=16, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MLP(hidden_channels=16)\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our MLP is defined by two linear layers and enhanced by [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html?highlight=relu#torch.nn.ReLU) non-linearity and [dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html?highlight=dropout#torch.nn.Dropout).\n",
    "Here, we first reduce the 1433-dimensional feature vector to a low-dimensional embedding (`hidden_channels=16`), while the second linear layer acts as a classifier that should map each low-dimensional node embedding to one of the 7 classes.\n",
    "\n",
    "Let's train our simple MLP by following a similar procedure as described in [the first part of this tutorial](https://colab.research.google.com/drive/1h3-vJGRVloF5zStxL5I0rSy4ZUPNsjy8).\n",
    "We again make use of the **cross entropy loss** and **Adam optimizer**.\n",
    "This time, we also define a **`test` function** to evaluate how well our final model performs on the test node set (which labels have not been observed during training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data.x)  # Perform a single forward pass.\n",
    "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "      model.eval()\n",
    "      out = model(data.x)\n",
    "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
    "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
    "      return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.9615\n",
      "Epoch: 002, Loss: 1.9557\n",
      "Epoch: 003, Loss: 1.9505\n",
      "Epoch: 004, Loss: 1.9423\n",
      "Epoch: 005, Loss: 1.9327\n",
      "Epoch: 006, Loss: 1.9279\n",
      "Epoch: 007, Loss: 1.9144\n",
      "Epoch: 008, Loss: 1.9087\n",
      "Epoch: 009, Loss: 1.9023\n",
      "Epoch: 010, Loss: 1.8893\n",
      "Epoch: 011, Loss: 1.8776\n",
      "Epoch: 012, Loss: 1.8594\n",
      "Epoch: 013, Loss: 1.8457\n",
      "Epoch: 014, Loss: 1.8365\n",
      "Epoch: 015, Loss: 1.8280\n",
      "Epoch: 016, Loss: 1.7965\n",
      "Epoch: 017, Loss: 1.7984\n",
      "Epoch: 018, Loss: 1.7832\n",
      "Epoch: 019, Loss: 1.7495\n",
      "Epoch: 020, Loss: 1.7441\n",
      "Epoch: 021, Loss: 1.7188\n",
      "Epoch: 022, Loss: 1.7124\n",
      "Epoch: 023, Loss: 1.6785\n",
      "Epoch: 024, Loss: 1.6660\n",
      "Epoch: 025, Loss: 1.6119\n",
      "Epoch: 026, Loss: 1.6236\n",
      "Epoch: 027, Loss: 1.5827\n",
      "Epoch: 028, Loss: 1.5784\n",
      "Epoch: 029, Loss: 1.5524\n",
      "Epoch: 030, Loss: 1.5020\n",
      "Epoch: 031, Loss: 1.5065\n",
      "Epoch: 032, Loss: 1.4742\n",
      "Epoch: 033, Loss: 1.4581\n",
      "Epoch: 034, Loss: 1.4246\n",
      "Epoch: 035, Loss: 1.4131\n",
      "Epoch: 036, Loss: 1.4112\n",
      "Epoch: 037, Loss: 1.3923\n",
      "Epoch: 038, Loss: 1.3055\n",
      "Epoch: 039, Loss: 1.2982\n",
      "Epoch: 040, Loss: 1.2543\n",
      "Epoch: 041, Loss: 1.2244\n",
      "Epoch: 042, Loss: 1.2331\n",
      "Epoch: 043, Loss: 1.1984\n",
      "Epoch: 044, Loss: 1.1796\n",
      "Epoch: 045, Loss: 1.1093\n",
      "Epoch: 046, Loss: 1.1284\n",
      "Epoch: 047, Loss: 1.1229\n",
      "Epoch: 048, Loss: 1.0383\n",
      "Epoch: 049, Loss: 1.0439\n",
      "Epoch: 050, Loss: 1.0563\n",
      "Epoch: 051, Loss: 0.9893\n",
      "Epoch: 052, Loss: 1.0508\n",
      "Epoch: 053, Loss: 0.9343\n",
      "Epoch: 054, Loss: 0.9639\n",
      "Epoch: 055, Loss: 0.8929\n",
      "Epoch: 056, Loss: 0.8705\n",
      "Epoch: 057, Loss: 0.9176\n",
      "Epoch: 058, Loss: 0.9239\n",
      "Epoch: 059, Loss: 0.8641\n",
      "Epoch: 060, Loss: 0.8578\n",
      "Epoch: 061, Loss: 0.7908\n",
      "Epoch: 062, Loss: 0.7856\n",
      "Epoch: 063, Loss: 0.7683\n",
      "Epoch: 064, Loss: 0.7816\n",
      "Epoch: 065, Loss: 0.7356\n",
      "Epoch: 066, Loss: 0.6951\n",
      "Epoch: 067, Loss: 0.7300\n",
      "Epoch: 068, Loss: 0.6939\n",
      "Epoch: 069, Loss: 0.7550\n",
      "Epoch: 070, Loss: 0.6864\n",
      "Epoch: 071, Loss: 0.7094\n",
      "Epoch: 072, Loss: 0.7238\n",
      "Epoch: 073, Loss: 0.7150\n",
      "Epoch: 074, Loss: 0.6191\n",
      "Epoch: 075, Loss: 0.6770\n",
      "Epoch: 076, Loss: 0.6487\n",
      "Epoch: 077, Loss: 0.6258\n",
      "Epoch: 078, Loss: 0.5821\n",
      "Epoch: 079, Loss: 0.5637\n",
      "Epoch: 080, Loss: 0.6368\n",
      "Epoch: 081, Loss: 0.6333\n",
      "Epoch: 082, Loss: 0.6434\n",
      "Epoch: 083, Loss: 0.5974\n",
      "Epoch: 084, Loss: 0.6176\n",
      "Epoch: 085, Loss: 0.5972\n",
      "Epoch: 086, Loss: 0.4690\n",
      "Epoch: 087, Loss: 0.6362\n",
      "Epoch: 088, Loss: 0.6118\n",
      "Epoch: 089, Loss: 0.5248\n",
      "Epoch: 090, Loss: 0.5520\n",
      "Epoch: 091, Loss: 0.6130\n",
      "Epoch: 092, Loss: 0.5361\n",
      "Epoch: 093, Loss: 0.5594\n",
      "Epoch: 094, Loss: 0.5049\n",
      "Epoch: 095, Loss: 0.5043\n",
      "Epoch: 096, Loss: 0.5235\n",
      "Epoch: 097, Loss: 0.5451\n",
      "Epoch: 098, Loss: 0.5329\n",
      "Epoch: 099, Loss: 0.5008\n",
      "Epoch: 100, Loss: 0.5350\n",
      "Epoch: 101, Loss: 0.5343\n",
      "Epoch: 102, Loss: 0.5138\n",
      "Epoch: 103, Loss: 0.5377\n",
      "Epoch: 104, Loss: 0.5353\n",
      "Epoch: 105, Loss: 0.5176\n",
      "Epoch: 106, Loss: 0.5229\n",
      "Epoch: 107, Loss: 0.4558\n",
      "Epoch: 108, Loss: 0.4883\n",
      "Epoch: 109, Loss: 0.4659\n",
      "Epoch: 110, Loss: 0.4908\n",
      "Epoch: 111, Loss: 0.4966\n",
      "Epoch: 112, Loss: 0.4725\n",
      "Epoch: 113, Loss: 0.4787\n",
      "Epoch: 114, Loss: 0.4390\n",
      "Epoch: 115, Loss: 0.4199\n",
      "Epoch: 116, Loss: 0.4810\n",
      "Epoch: 117, Loss: 0.4484\n",
      "Epoch: 118, Loss: 0.5080\n",
      "Epoch: 119, Loss: 0.4241\n",
      "Epoch: 120, Loss: 0.4745\n",
      "Epoch: 121, Loss: 0.4651\n",
      "Epoch: 122, Loss: 0.4652\n",
      "Epoch: 123, Loss: 0.5580\n",
      "Epoch: 124, Loss: 0.4861\n",
      "Epoch: 125, Loss: 0.4405\n",
      "Epoch: 126, Loss: 0.4292\n",
      "Epoch: 127, Loss: 0.4409\n",
      "Epoch: 128, Loss: 0.3575\n",
      "Epoch: 129, Loss: 0.4468\n",
      "Epoch: 130, Loss: 0.4603\n",
      "Epoch: 131, Loss: 0.4108\n",
      "Epoch: 132, Loss: 0.4601\n",
      "Epoch: 133, Loss: 0.4258\n",
      "Epoch: 134, Loss: 0.3852\n",
      "Epoch: 135, Loss: 0.4028\n",
      "Epoch: 136, Loss: 0.4245\n",
      "Epoch: 137, Loss: 0.4300\n",
      "Epoch: 138, Loss: 0.4693\n",
      "Epoch: 139, Loss: 0.4314\n",
      "Epoch: 140, Loss: 0.4031\n",
      "Epoch: 141, Loss: 0.4290\n",
      "Epoch: 142, Loss: 0.4110\n",
      "Epoch: 143, Loss: 0.3863\n",
      "Epoch: 144, Loss: 0.4215\n",
      "Epoch: 145, Loss: 0.4519\n",
      "Epoch: 146, Loss: 0.3940\n",
      "Epoch: 147, Loss: 0.4429\n",
      "Epoch: 148, Loss: 0.3527\n",
      "Epoch: 149, Loss: 0.4390\n",
      "Epoch: 150, Loss: 0.4212\n",
      "Epoch: 151, Loss: 0.4128\n",
      "Epoch: 152, Loss: 0.3779\n",
      "Epoch: 153, Loss: 0.4801\n",
      "Epoch: 154, Loss: 0.4130\n",
      "Epoch: 155, Loss: 0.3962\n",
      "Epoch: 156, Loss: 0.4262\n",
      "Epoch: 157, Loss: 0.4210\n",
      "Epoch: 158, Loss: 0.4081\n",
      "Epoch: 159, Loss: 0.4066\n",
      "Epoch: 160, Loss: 0.3782\n",
      "Epoch: 161, Loss: 0.3836\n",
      "Epoch: 162, Loss: 0.4172\n",
      "Epoch: 163, Loss: 0.3993\n",
      "Epoch: 164, Loss: 0.4477\n",
      "Epoch: 165, Loss: 0.3714\n",
      "Epoch: 166, Loss: 0.3610\n",
      "Epoch: 167, Loss: 0.4546\n",
      "Epoch: 168, Loss: 0.4387\n",
      "Epoch: 169, Loss: 0.3793\n",
      "Epoch: 170, Loss: 0.3704\n",
      "Epoch: 171, Loss: 0.4286\n",
      "Epoch: 172, Loss: 0.4131\n",
      "Epoch: 173, Loss: 0.3795\n",
      "Epoch: 174, Loss: 0.4230\n",
      "Epoch: 175, Loss: 0.4139\n",
      "Epoch: 176, Loss: 0.3586\n",
      "Epoch: 177, Loss: 0.3588\n",
      "Epoch: 178, Loss: 0.3911\n",
      "Epoch: 179, Loss: 0.3810\n",
      "Epoch: 180, Loss: 0.4203\n",
      "Epoch: 181, Loss: 0.3583\n",
      "Epoch: 182, Loss: 0.3690\n",
      "Epoch: 183, Loss: 0.4025\n",
      "Epoch: 184, Loss: 0.3920\n",
      "Epoch: 185, Loss: 0.4369\n",
      "Epoch: 186, Loss: 0.4317\n",
      "Epoch: 187, Loss: 0.4911\n",
      "Epoch: 188, Loss: 0.3369\n",
      "Epoch: 189, Loss: 0.4945\n",
      "Epoch: 190, Loss: 0.3912\n",
      "Epoch: 191, Loss: 0.3824\n",
      "Epoch: 192, Loss: 0.3479\n",
      "Epoch: 193, Loss: 0.3798\n",
      "Epoch: 194, Loss: 0.3799\n",
      "Epoch: 195, Loss: 0.4015\n",
      "Epoch: 196, Loss: 0.3615\n",
      "Epoch: 197, Loss: 0.3985\n",
      "Epoch: 198, Loss: 0.4664\n",
      "Epoch: 199, Loss: 0.3714\n",
      "Epoch: 200, Loss: 0.3810\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, we can call the `test` function to see how well our model performs on unseen labels.\n",
    "Here, we are interested in the accuracy of the model, *i.e.*, the ratio of correctly classified nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5900\n"
     ]
    }
   ],
   "source": [
    "test_acc = test()\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see, our MLP performs rather bad with only about 59% test accuracy. But why does the MLP do not perform better? The main reason for that is that this model suffers from heavy overfitting due to only having access to a small amount of training nodes, and therefore generalizes poorly to unseen node representations.\n",
    "\n",
    "It also fails to incorporate an important bias into the model: Cited papers are very likely related to the category of a document. That is exactly where Graph Neural Networks come into play and can help to boost the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
